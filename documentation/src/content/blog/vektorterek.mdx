---
title: 'BevezetÃ©s a vektorterekbe'
description: 'A vektorterek axiomatikus felÃ©pÃ­tÃ©se, lineÃ¡ris kombinÃ¡ciÃ³, fÃ¼ggetlensÃ©g Ã©s bÃ¡zis.'
pubDate: 'Jan 02, 2026'
heroImage: '../../assets/sponge.png'
tags: ['math', 'linear-algebra', 'vectors']
---

import Question from '../../components/Question.astro';
import ProofBox from '../../components/ProofBox.astro';
import AxiomBox from '../../components/AxiomBox.astro';

## Vektorterek

ğŸ‘¨â€ğŸ«: Elerkeztunk egy ujabb fontos reszhez. Mit tanultatok a kozepiskolaban a vektorokrol?

ğŸ™‹â€â™‚ï¸: Hat a vektor az ilyen nyilacska a sikon vagy a terben. De kicsit formalisabban olyasmit mondtunk hogy egy iranyitott szakasz, vagy tombbe rendezett szamok.

ğŸ‘¨â€ğŸ«: Igen, igen valoban lehet igy reprezentalni az $\mathbb{R}^{2}$-ben vagy $\mathbb{R}^{3}$-ban lako vektorokat. Emellett matematikusok megalkottak egy szabalyrendszert ami kiszelesiti a vektorok fogalmat. Gondolhatsz ugy is a dologra hogy megfigyeljuk hogy a terben lako nyilacskakra milyen szabalyok igazak, osszegyujtjuk ezeket a szabalyokat, es azok a dolgoka mikre ezek a szabalyok igazak azokat vektornak fogjuk hivni. Vagyis pontosabban azt a halmazt amire igazak ezek a szabalyok azt vektorternek, a halmaz egy elemet pedig vektornak. Van otleted hogy milyen szabalyokbol alkotjak a vektorter axiomait?

ğŸ™‹â€â™‚ï¸: Gondolom ossze kell tudjunk adni a vektorokat. Az is jo lenne ha tudnank oket megnyujtani, tehat skalarral szorozni. Meg amugy tudtunk szorozni is vektorokat a "rendes" terben.

ğŸ‘¨â€ğŸ«: Ezek igen jo gondolatok. Kettot kapasbol bele is rakunk a vektorter axiomaiba. Egyet viszont ki fogunk hagyni. 

<Question
  question="Szerinted melyik szabalyt hagyjuk ki a vektorterek axiomai kozul?"
  options={[
    "vektorok osszeadasa",
    "vektorok megnyujtasa (skalarral valo szorzas)",
    "ket vektor osszeszorzasa"
  ]}
  correctAnswerIndex={2}
/>

{/* todo le kell irni mi a test es mi a csopi */}



<AxiomBox title="VektortÃ©r AxiÃ³mÃ¡k">

Legyen egy $K$ test (Ã¡ltalÃ¡ban $\mathbb{R}$ vagy $\mathbb{C}$). Azt mondhatjuk, hogy $V$ egy vektortÃ©r $K$ felett, ha: (az alÃ¡bbiakban $u,v,w\in V$ Ã©s $\lambda,\mu\in K$)

**Ã–sszeadÃ¡s**

Legyen
$$
+ : V\times V \to V,\qquad (u,v)\mapsto u+v.
$$
A kÃ¶vetkezÅ‘ tulajdonsÃ¡gok teljesÃ¼lnek:
$$
\begin{aligned}
1. \quad & u+(v+w)=(u+v)+w && (\text{asszociatÃ­v}) \\
2. \quad & u+v=v+u && (\text{kommutatÃ­v}) \\
3. \quad & \exists\,0\in V \text{ olyan, hogy } 0+v=v \quad \forall v\in V \text{ esetÃ©n} && (\text{nullvektor}) \\
4. \quad & \forall v\in V\text{-hez } \exists -v\in V \text{ Ãºgy, hogy } v+(-v)=0 && (\text{ellenkezÅ‘ elem})
\end{aligned}
$$

**SkalÃ¡rral valÃ³ szorzÃ¡s**

Legyen
$$
\cdot : K\times V \to V,\qquad (\lambda,v)\mapsto \lambda v.
$$
A kÃ¶vetkezÅ‘ axiÃ³mÃ¡k teljesÃ¼lnek:
$$
\begin{aligned}
1. \quad & \lambda(\mu v)=(\lambda\mu)v && (\text{ez nem pont asszociativitÃ¡s de hasonlÃ³}) \\
2. \quad & \exists\,1\in K \text{ Ãºgy, hogy } 1\cdot v = v \quad \forall v\in V \text{ esetÃ©n} && \\
3. \quad & (\lambda+\mu)v=\lambda v + \mu v && (\text{disztributivitÃ¡s a skalÃ¡rok Ã¶sszeadÃ¡sÃ¡ra}) \\
4. \quad & \lambda(v+u)=\lambda v + \lambda u && (\text{disztributivitÃ¡s a vektorok Ã¶sszeadÃ¡sÃ¡ra})
\end{aligned}
$$

</AxiomBox>

**PÃ©ldÃ¡k vektorterekre**

ğŸ‘¨â€ğŸ«: Probald meg belatni hogy az alabbi halmazok vektorteret alkotnak.

*   $\mathbb{R}^n$, $\mathbb{C}^n$
*   $C(\mathbb{R},\mathbb{R})$ (folytonos fÃ¼ggvÃ©nyek tere)
*   Sorozatok tere (mindenÃ¼tt definiÃ¡lt Ã¶sszeadÃ¡s Ã©s skalÃ¡rszorzÃ¡s){/* ez itt miez */}
*   $\{0\}$ (triviÃ¡lis vektortÃ©r)
{/* todo ezt itt meg nagyon szepiteni kene */}
## LineÃ¡ris kombinÃ¡ciÃ³

ğŸ‘¨â€ğŸ«: Mivel a vektorter zart az osszeadasra, igy az $I$ indexhalmazon
1. $\forall i\in I:\; u_i\in V$,
2. $\forall i\in I:\; \lambda_i\in K$,

akkor
$$
v=\sum_{i \in I}\lambda_i u_i \in V.
$$
Ekkor azt mondjuk, hogy $\sum_{i\in I}\lambda_i u_i$ az $(u_i)_{i\in I}$ vektorok valamely lineÃ¡ris kombinÃ¡ciÃ³ja.

## LineÃ¡ris fÃ¼ggetlensÃ©g

Az $(u_i)_{i\in I}$ vektorrendszer elemei lineÃ¡risan fÃ¼ggetlenek, ha
$$
\sum_{i\in I}\lambda_i u_i = 0 \quad\Longleftrightarrow\quad \forall i\in I:\; \lambda_i = 0.
$$
SzÃ³ban: az $(u_i)_{i\in I}$ vektorrendszer elemei akkor Ã©s csak akkor lineÃ¡risan fÃ¼ggetlenek, ha a nullvektort elÅ‘Ã¡llÃ­tÃ³ lineÃ¡ris kombinÃ¡ciÃ³ban szereplÅ‘ Ã¶sszes egyÃ¼tthatÃ³ nulla.

<div id="linear-independence-container" style={{display: 'flex', flexDirection: 'column', alignItems: 'center', margin: '2rem 0'}}></div>
<script is:inline src="/libraries/p5.min.js"></script>
<script is:inline src="/linear_independence_sim.js"></script>

**Feladatok**

1. VizsgÃ¡ljuk, hogy egyetlen vektor $v\in V$ lineÃ¡risan fÃ¼ggetlen-e.
2. TegyÃ¼k fel, hogy $V=\mathbb{R}^2$. VizsgÃ¡ljuk, hogy a kÃ¶vetkezÅ‘ kÃ©t vektor lineÃ¡risan fÃ¼ggetlen-e:
   $$
   a=(0,1),\qquad b=(1,1).
   $$

## BÃ¡zis, dimenzio

Egy $V$ vektortÃ©rben az $(u_i)_{i\in I}$ vektorrendszer bÃ¡zis, ha
1. $(u_i)_{i\in I}$ lineÃ¡risan fÃ¼ggetlen,
2. minden $x\in V$ elÅ‘Ã¡llÃ­thatÃ³ az $(u_i)_{i\in I}$ lineÃ¡ris kombinÃ¡ciÃ³jakÃ©nt, azaz $\operatorname{span}\{u_i:i\in I\}=V$.

**ÃllÃ­tÃ¡s**: Minden vektortÃ©rben van bÃ¡zis.
{/*todo na ezt en nem igy irtam, es kiegesziteni */}
{/* todo vektorok felirhatoak a bazisvektorokkal egyertelmuen */}

ğŸ‘¨â€ğŸ«: Egy vektorter dimenzioja a vektorterben levo bazisok szamossaga. Tehat peldaul.: $ dim(\mathbb{R}^{3}) = 3 $

## Linearis alter, linearis burok

ğŸ‘¨â€ğŸ«: Vegyuk az ($\mathbb{R}^3$) teret. Errol mar belattuk, hogy vektorteret alkot. Vegyuk ehhez a szokasos, standard $ x, y, z $ koordinatazast. Most vegyunk azonak a vektoroknak a halmazat amelyek $ [a, b, 0] $ alakba irhatoak. Mi tortenik ha ket ilyen vektort osszeadunk?

ğŸ™‹â€â™‚ï¸: Hat tovabbra is az $ xy $ sikban maradnak.

ğŸ‘¨â€ğŸ«: Pontosan. Es ha egy ilyen vektort megszorzunk egy skalarral?

ğŸ™‹â€â™‚ï¸: Szinten az $ xy $ sikban marad a modositott vektor.

ğŸ‘¨â€ğŸ«: Ezek a vektorok mar eredetileg is egy vektorter elemei voltak. Igy mit mondhatunk el ezeknek az $ [a, b, 0] $ alaku vektorok halmazarol?

ğŸ™‹â€â™‚ï¸: Hat igazak rajuk a vektorter axiomai. Es valahogy igy nem tudnak "kimenni" az $ xy $ sikrol. Szoval azt gondolom hogy ok is egyfajta kisebb vektorteret alkotnak.

cNagyon jooo. Ezt linearis alternek nevezzuk. $ L \subseteq V $ linearis altere a $V$ vektorterunknek, ha:

$$
\begin{aligned}
1. \quad & a, b \in L \implies a + b \in L \\
2. \quad & a \in L, \lambda \in K \implies \lambda a \in L
\end{aligned}
$$

{/* todo ide kep az egyenesrol es arrol h subset */}

ğŸ™‹â€â™‚ï¸: Szoval a null vektor mindig bennevan a linaris alberben, es az egesz vektorter mindig egy linaris alter is egyben, ha jol ertem

ğŸ‘¨â€ğŸ«: Igen, igen, nagyon jÃ³ Ã©szrevÃ©telek. Haladjunk tovÃ¡bb a lineÃ¡ris burok fogalmÃ¡ra. Ha van egy $V$ vektorterÃ¼nk Ã©s abban egy $A = \{v_1, v_2, \dots, v_n\}$ vektorrendszerÃ¼nk, akkor $A$ lineÃ¡ris burka ($\langle A \rangle$):

$$
\langle A \rangle = \left\{ \sum_{i=1}^{n} \lambda_i v_i : \lambda_i \in \mathbb{K}, n \in \mathbb{N} \right\}
$$

ğŸ™‹â€â™‚ï¸: Ez a burok linaris burok dolog igazabol linearis alteret konstrual, nem?

ğŸ‘¨â€ğŸ«: Igen. Ha osszeadod ket linearis kombinaciojat a vektorrendeszernek akkor az meg mindig a vektrorrendszered linearis kombinacioja lesz. Ugyanez igaz a szorzasra is. Tehat valoban, a linaris burok elkeszitese egy olyan "gepezet" ami a vektorrendszerunkre illeszt egy linaris alteret. De meg az is igaz hogy a leheto legkisebb linaris alteret illeszti az eredeti vektorainkra. Ezt ertheted ugy, hogy ha van a 3D-s terben ket vektorod akkor azoknak a linaris burka egy sikot fog meghatarozni nem pedig az egesz 3D-s teret.






## SkalÃ¡rszorzatos vektorterek

ğŸ™‹â€â™‚ï¸: VÃ¡rjunk csak, a 3 dimenziÃ³s tÃ©rben tudtunk vektorokat szorozni pl.: skalÃ¡risan vagy vektoriÃ¡lisan. Ezt a tulajdonsÃ¡got nem Ã¡ltalÃ¡nosÃ­tottuk?

ğŸ‘¨â€ğŸ«: Ez egy nagyon Ã¼gyes Ã©szrevÃ©tel! (jÃ¡r Ã©rte egy csoki) A vektorterekre adott Ã¡ltalÃ¡nos definÃ­ciÃ³nk valÃ³ban nem foglalja magÃ¡ba a vektorok szorzÃ¡sÃ¡t. Azonban egyes vektorterekben lÃ©tezik skalÃ¡ris szorzÃ¡s ezeket a vektortereket -- nagyon meglepÅ‘ mÃ³don -- skalÃ¡rszorzatos vektortereknek nevezzÃ¼k. PÃ©ldÃ¡ul ahogy lÃ¡ttuk a sÃ­k ($\mathbb{R}^2$) vektorteret alkot (a valÃ³s szÃ¡mtest fÃ¶lÃ¶tt), Ã©s a sÃ­kon 2 vektornak lÃ©tezik skalÃ¡ris szorzata.

ğŸ™‹â€â™‚ï¸: Hmm... Es mi a helyzet a folytonos fÃ¼ggvÃ©nyekkel azok is vektorteret alkotnak, nem? Ott nem tudom csak Ãºgy Ã¶sszeszorozni a vektorok komponenseit mint egy ($\mathbb{R}^n$)-es vektortÃ©rben. Akkor itt nincs skalÃ¡ris szorzÃ¡s?

ğŸ‘¨â€ğŸ«: Igen igen, ez egy nehÃ©zsÃ©g. Ha emlÃ©kszel a kÃ¶zÃ©piskolÃ¡ban tanult vektorrÃ³l alkotott kÃ©p valami olyasmi volt, hogy egy sÃ­kon vagy egy tÃ©rben lakÃ³ nyilacska vagy egymÃ¡s alÃ¡ pakolt szÃ¡mok. Amikor bevezettÃ¼k a vektortereket valahogy megvizsgÃ¡ltuk, hogy milyen belsÅ‘ tulajdonsÃ¡gokkal bÃ­rnak a ezek a vektorok. Ezeket a tulajdonsÃ¡gokat megtartottuk Ã©s ezekbÅ‘l megalkottuk a vektortÃ©r axiÃ³mÃ¡it. Most valami hasonlÃ³t fogunk csinÃ¡lni a skalÃ¡rszorzÃ¡ssal is. Vajon milyen tulajdonsÃ¡gai vannak a sÃ­kon vett standard skalÃ¡rszorzÃ¡snak?

ğŸ™‹â€â™‚ï¸: Gondolkozzunk... VegyÃ¼nk kÃ©t vektort, mondjuk $a$-t Ã©s $b$-t. 1. Ha $a$ hosszÃ¡t 2-szeresÃ©re nÃ¶velem akkor a skalÃ¡rszorzat is kÃ©tszer akkora lesz. Ezt geometriailag lÃ¡thatjuk. 2. Ha veszÃ¼nk egy harmadik vektort, mondjuk $c$-t, akkor ha $a + b$-t skalÃ¡risan szorozzuk $c$-vel az igazÃ¡bÃ³l ugyan olyan mintha elÅ‘szÃ¶r $a$-t szoroztuk volna $c$-vel Ã©s aztÃ¡n ehhez hozzÃ¡adtuk volna $b$ Ã©s $c$ saklÃ¡rszorzatÃ¡t. Ezt pedig onnÃ©t tudhatjuk, hogy ha kibontjuk a skalÃ¡rszorzatot komponensekre akkor ott mÃ¡r a szorzÃ¡s disztributÃ­v a valÃ³s szÃ¡mokon Ã©s utÃ¡na Ã¡trendezzÃ¼k az Ã¶sszeget. 3. Hogyha $a$-t Ã¶nmagÃ¡val skalÃ¡rszorozzuk akkor biztos, hogy egy pozitÃ­v szÃ¡mot kapunk, feltÃ©ve persze, hogy $a$ nem nullvektor. Hmmm... nem tudom mi van mÃ©g. Ja igen talÃ¡n mÃ©g egy. 4. FelcserÃ©lhetjÃ¼k a szorzÃ¡s sorrendjÃ©t Ã©s ugyan azt az eredmÃ©nyt kapjuk. TehÃ¡t $a$ szor $b$ = $b$ szer $a$

ğŸ‘¨â€ğŸ«: Wooow, nagyon jo Ã¼gyesen megfigyelted ezeket a tulajdonsÃ¡gokat! Persze egy kis kiegÃ©szÃ­tÃ©sre azÃ©rt szÃ¼ksÃ©g lesz, ha a vektorterÃ¼nket esetleg komplex szÃ¡mtest felett akarjuk definiÃ¡lni, de alapvetÅ‘en elÃ©g hasonlÃ³ marad a szitu ahhoz amit mondtÃ¡l. (HasznÃ¡ljuk a $ \langle \cdot, \cdot \rangle $ jelÃ¶lÃ©st.)

{/* todo ide johetne kÃ©rdÃ©s */}

SkalÃ¡rszorzat feltÃ©tele egy vektortÃ©rben:

1. $ \langle a, b \rangle = \overline{\langle b, a \rangle} $
2. $ \langle \lambda a, b \rangle = \bar{\lambda} \langle a, b \rangle $
3. $ \langle a + b, c \rangle = \langle a, c \rangle + \langle b, c \rangle $
4. $ \langle a, a \rangle > 0 $ ha a nem a nullvektor
5. $ \langle a, a \rangle = 0 $ ha a a nullvektor

ğŸ™‹â€â™‚ï¸: OkÃ©, hogy segÃ­t ez nekÃ¼nk a folytonos fÃ¼ggvÃ©nyeken Ã©rtelmezni a skalÃ¡rszorzÃ¡st? NÃ©zhetnÃ©nk erre egy pÃ©ldÃ¡t?

ğŸ‘¨â€ğŸ«: Azt mÃ¡r korÃ¡bban belÃ¡ttuk, hogy a folytonos fÃ¼ggvÃ©nyek a $[0, 1]$ intervallumon vektorteret alkotnak. tekintsÃ¼k a kÃ¶vetkezÅ‘ skalÃ¡ris szorzÃ¡st:

$ \langle f, g \rangle = \int_{0}^{1}f(x)g(x) \, dx $

ha lecsekkolod a fent emlÃ­tett kritÃ©riumokat rÃ¡jÃ¶ssz, hogy ez egy valid skalÃ¡ris szorzÃ¡s ezen a vektortÃ©ren.
{/* todo ezt itt lehet levezethetnem */}


## Caucy-Schwarz-Bunyakowskij egyenlotlenseg es vektorok altal bezart szog

ğŸ‘¨â€ğŸ«: Vegyunk a megszokott sikon ket vektort. Itt mindenki szamara egyertelmu hogy mi a ket vektor altal bezart szog. Akar foghatunk egy szogmerot is es leolvashatjuk a szoget. Na de vegyuk most a folytonos fuggvenyek vektorteret. Itt mi a helyzet, hogy hatarozzuk meg ebben a vektorterben lako ket vektor altal bezart szoget?

ğŸ™‹â€â™‚ï¸: Ha jol emlekszem a szog valahogy kapcsolatban allt a skalarszorzassal. Ki tudnank esetleg azt hasznalni?

ğŸ‘¨â€ğŸ«: Nem is rossz otlet! Kicsit tavolabbrol fogok elindulni de vegul ide fogunk kilyukadni. Ha van egy skalarszorzatos vektorterunk akkor abban mindig fennall egy nagyon fontos egyenlotlenseg.:

$$
| \langle x, y \rangle | \leq \|x\| \|y\|
$$

(Itt $ \|x\| := \sqrt{\langle x, x \rangle} $ kesobb be fogjuk latni, hogy $ \|x\| $ a skalarszorzat altal indukalt normaja $ x $-nek. Most egyelore viszont csak egy jelolesbeli egyszerusites.) Ez egy tok hasznos egyenlotlenseg. Ezt fogjuk hasznalni a vektorok altal bezart szog definialasara is mellesleg. Lenyegileg annyit mont ki, hogy ha van ket vektorunk akkor azoknak a skalarszorzata biztos, hogy kisebb vagy egyenlo, mint ha a ket vektort onmagukkal skalarszorozzuk, es aztan a skalarszorzatokat osszeszorozzuk.

{/* todo ami ezalatt ki van kommentelve az nem teljesen igaz */}
{/* 
ğŸ™‹â€â™‚ï¸: Ahaa. Eeees akarmilyen normat hasznalhatok? Peldaul a p-normak kozul megvalaszthatom a p erteket 14-nek?

ğŸ‘¨â€ğŸ«: Nagyon jo kerdes! Itt fontos megemliteni, hogy mindig a skalarszorzas altal indukalt normat hasznaljuk az egyenlotlensegben. Kiprobalhatod peldaul a sikon az $ (1, 1), (2, 2) $ vektorokon a szokasos skalarszorzassal es a vegtelen normaval. Nem fog mukodni az egyenlotlenseg. Na de ha mar itt vagyunk lassuk is be gyorsan ezt az egyenlotlenseget! */}

<ProofBox title="C.S.B. bizonyitas">
$$
\begin{aligned}
& \|x - \lambda y\| \ge 0 \\
& (\text{ahol } \forall x, y \in V, \forall \lambda \in K, y \neq 0) \\
\\
& \text{A skalarszorzatot visszairva:} \\
& \langle x - \lambda y, x - \lambda y \rangle \ge 0 \\
\\
& \text{Kibontva:} \\
& \langle x, x \rangle - \langle x, \lambda y \rangle - \langle \lambda y, x \rangle + \langle \lambda y, \lambda y \rangle \ge 0 \\
& \|x\|^2 - \lambda \langle x, y \rangle - \bar{\lambda} \langle y, x \rangle + |\lambda|^2 \|y\|^2 \ge 0 \\
\\
& \text{Mivel minden } \lambda\text{-ra igaz az egyenlÅ‘tlensÃ©g,} \\
& \text{Ã­gy legyen: } \lambda = \frac{\langle y, x \rangle}{\|y\|^2} \\
\\
& \text{BehelyettesÃ­tve:} \\
& \|x\|^2 - \frac{\langle y, x \rangle \langle x, y \rangle}{\|y\|^2} - \frac{\overline{\langle y, x \rangle}\langle y, x \rangle}{\|y\|^2} + \frac{|\langle y, x \rangle|^2 \|y\|^2}{\|y\|^4} \ge 0 \\
\\
& \text{KihasznÃ¡ljuk a komplex szÃ¡m Ã©s konjugÃ¡ltjÃ¡ra} \\
& \text{vonatkozÃ³ Ã¶sszefÃ¼ggÃ©st: } (z \bar{z} = |z|^2) \\
& \|x\|^2 - \frac{|\langle y, x \rangle|^2}{\|y\|^2} - \frac{|\langle y, x \rangle|^2}{\|y\|^2} + \frac{|\langle y, x \rangle|^2}{\|y\|^2} \ge 0 \\
\\
& \text{Ã–sszevonÃ¡s utÃ¡n:} \\
& \|x\|^2 - \frac{|\langle y, x \rangle|^2}{\|y\|^2} \ge 0 \\
& \|x\|^2 \ge \frac{|\langle y, x \rangle|^2}{\|y\|^2} \\
& \|x\|^2 \|y\|^2 \ge |\langle x, y \rangle|^2 \\
\\
& \text{GyÃ¶kvonÃ¡s utÃ¡n:} \\
& |\langle x, y \rangle| \le \|x\| \|y\|
\end{aligned}
$$
</ProofBox>


## Normak

ğŸ‘¨â€ğŸ«: Ahogy a skalarszorzast is altalanositanunk kellett ha kicsit elborultabb vektorterekkel dolgoztunk, igy a vektorok hosszat sem olyan konnyu mar lemerjuk. Hasonloan itt is be fogunk vezetni egyes axiomakat. Es inkabb a norma szot fogjuk hasznalni a hossz helyett. Ha visszagondolsz a skalarszorzat axiomaira, milyen felteteleket kene szabjunk, hogy altalanosithassuk a vektor hosszat?

ğŸ™‹â€â™‚ï¸: Szerintem itt is hasznos lenne ha nem engednenk meg hogy egy vektor negativ hosszu legyen. Meg hogyha a vektort megszorozzuk 5-tel akkor a normaja is lehetne 5-szor akkora.

ğŸ‘¨â€ğŸ«: Jo otlet! Meg egy kicsit kiegeszitem es mar meg is van a norma definicioja.
$$
\begin{aligned}
\|\cdot\| : V \to \mathbb{R} \\
1. \quad & \|x\| \ge 0 \\
2. \quad & \|x\| = 0 \iff x = 0 \\
3. \quad & \|\lambda x\| = |\lambda| \|x\| \\
4. \quad & \|x + y\| \le \|x\| + \|y\|
\end{aligned}
$$

Szoval a norma egy olyan fuggveny ami a vektorterbol kepez a valos szamokba. Es valoban, a norma mindig nagyobb mint 0 kiveve ha magat a 0 vektort normaljuk. Abban is igazad volt, hogy ki kell tudjuk hozni a skalar szorzot. Itt annyi pontositast tettem hogy a skalar abszoluterteket kell tudjuk kiemelni. Ez azert kell mert lehet, hogy a  komplex kest folott van definialva a vektorterunk. Amit viszont elfelejtettel emliteni az a haromszogegyenlotlenseg. Ha van kedved leellenorizheted, hogy a sikon hasznalt standard norma kielegiti a normara adott felteteleinket. ( $ \|x\| = \sqrt{x_1^{2} + x_2^{2}} $ )

ğŸ™‹â€â™‚ï¸: Hmmm... A normaban es a skalarszorzasban latszolag eleg sok kozos tulajdonsag van. Nem tudjuk valahogy oket osszekotni?

ğŸ‘¨â€ğŸ«: Jo meglatas. Pont ezzel fogjuk folytatni. Ha van egy skalarszorzatos vektorterunk akkor abban a skalarszorzas normat indukal.:
$$
\|x\| = \sqrt{\langle x, x \rangle}
$$

Es itt mar $ \|x\| $-el a normat jeloljuk. Nezzuk meg a bizonyitast!

Az hogy nem negativ kapasbol kovetkezik a skalarszorzas axiomaibol (1. 2.) pont. A $ | \lambda |$ -val valo kiemeles is egesz gyorsan belathato. A haromszogegyenlotlenseg viszont egy kicsit trukkosebb.

<ProofBox title="BizonyÃ­tÃ¡s: HÃ¡romszÃ¶g-egyenlÅ‘tlensÃ©g">
$$
\begin{aligned}
& \text{TegyÃ¼k fel, hogy igaz a hÃ¡romszÃ¶g-egyenlÅ‘tlensÃ©g:} \\
& \|x+y\| \le \|x\| + \|y\| \\
\\
& \text{A skalÃ¡rszorzÃ¡s tulajdonsÃ¡gai miatt nÃ©gyzetreemelhetÃ¼nk:} \\
& \langle x+y, x+y \rangle \le \\
& \le \langle x,x \rangle + \langle y,y \rangle + 2\sqrt{\langle x,x \rangle}\sqrt{\langle y,y \rangle} \\
\\
& \text{A bal oldalt kibontva:} \\
& \langle x,x \rangle + \langle x,y \rangle + \langle y,x \rangle + \langle y,y \rangle \le \\
& \le \langle x,x \rangle + \langle y,y \rangle + 2\sqrt{\langle x,x \rangle}\sqrt{\langle y,y \rangle} \\
\\
& \text{EgyszerÅ±sÃ­tve } \langle x,x \rangle \text{ Ã©s } \langle y,y \rangle \text{ tagokkal:} \\
& \langle x,y \rangle + \langle y,x \rangle \le 2\sqrt{\langle x,x \rangle}\sqrt{\langle y,y \rangle} \\
\\
& \text{KihasznÃ¡lva a skalÃ¡rszorzÃ¡s tulajdonsÃ¡gait } (\langle y,x \rangle = \overline{\langle x,y \rangle}) \\
& \text{Ã©s a komplex szÃ¡mokÃ©t } (z + \bar{z} = 2\text{Re}(z)): \\
& 2\text{Re}(\langle x,y \rangle) \le 2\sqrt{\langle x,x \rangle}\sqrt{\langle y,y \rangle} \\
\\
& \text{Mivel } \text{Re}(z) \le |z|, \text{ ezÃ©rt:} \\
& |\langle x,y \rangle| \le \sqrt{\langle x,x \rangle}\sqrt{\langle y,y \rangle} \\
\\
& \text{Ez viszont a C.S.B. egyenlÅ‘tlensÃ©g miatt igaz,} \\
& \text{Ã­gy az Ã¡llÃ­tÃ¡st belÃ¡ttuk.}
\end{aligned}
$$
</ProofBox>






### P-normak

ğŸ‘¨â€ğŸ«: A $ \mathbb{K}^n $ vektortereken van a normaknak egy kulonleges csaladja. Ezeket p-normaknak hivjuk (eleg beszedes elnevezes).:

$$
\|x\|_{p} = \left(\sum_{i=1}^{n} |x_{i}|^{p} \right)^{\frac{1}{p}}
$$

Fontos megemliteni, hogy az allitas csak $ p \geq 1 $ -re ad valid normat. Nezd meg hogy mire egyszerusodik ha $ p=2, n=2$ visszakapjuk a rendes sikon a tavolsag meresehez szokasosan hasznalt formulat. Mint ahogy sok mas dolog is amit idaig csinaltunk egyfajta kiterjesztese a megszokott fogalmainkrol. Az alabbi animacioban megnezheted, hogy hogy nez ki egy egysegkor ha a sikon a tavolsagot nem $ p = 2$-es norma szerint mernenk.

<div id="p-norm-container" style={{display: 'flex', flexDirection: 'column', alignItems: 'center', margin: '2rem 0'}}></div>
<script is:inline src="/p_norm_sim.js"></script>

ğŸ™‹â€â™‚ï¸: Na de hogyan bizonyitjuk, hogy ezek a p-normak valoban kielegitik a normak axiomait?

ğŸ‘¨â€ğŸ«: Az hogy a p-norma nem mehet nulla ala azt maga a p-norma definicioja biztositja. A skalar kiemelese is konnyeden belathato. Megint a haromszogegyenlotlenseg a nehezseg. Ehhez felhasznaljuk az un. Minkowski egyenlotlenseget ami azt mondja ki, hogy:

$$
\left(\sum_{i=1}^{n} |x_{i} + y_{i}|^{p} \right)^{\frac{1}{p}} \leq \left(\sum_{i=1}^{n} |x_{i}|^{p} \right)^{\frac{1}{p}} + \left(\sum_{i=1}^{n} |y_{i}|^{p} \right)^{\frac{1}{p}}
$$

amennyiben $ p \geq 1 $. Ebbol pedig kovetkezik, hogy a p-normak valodi normak.

ğŸ™‹â€â™‚ï¸: Mi tortenik ha p-vel tartunk a vegtelenbe?

ğŸ‘¨â€ğŸ«: Igen jo kerdes. Ha a $p$ tart vegtelen esetet vizsgaljuk akkor belathato hogy a kovetkezo formulat kapjuk:

$$
\|x\|_{\infty} = \max \{ |x_1|, \dots, |x_n| \}
$$

### Vektorok altal bezart szog

ğŸ‘¨â€ğŸ«: Ha emlekszel meg a C.S.B. egyenlotlensegnel belattuk, hogy $ |\langle x, y \rangle|^{2} \leq \|x\|^{2} \|y\|^{2} $ Rendezzuk at egy kicsit a dolgokat!

$$
\begin{aligned}
& \frac{ |\langle x, y \rangle|^{2} }{\|x\|^{2} \|y\|^{2}} \leq 1 \\
\\
& -1 \leq \frac{ |\langle x, y \rangle| }{\|x\| \|y\|} \leq 1
\end{aligned}
$$

ğŸ‘¨â€ğŸ«: Emlekszel meg, hogy a kozepsuliban hogy definialtak a skalarszorzatot?

ğŸ™‹â€â™‚ï¸: Igen. Ket vektor skalarszorzata a vektorok hosszanak szorzata, es ezt meg megszorozzuk a ket vektor altal bezart szok koszinuszaval. Szoval valami ilyesmi kepletet kaptunk:

$$
x \cdot y = |x||y|cos(\alpha)
$$

Varjunk csak ez tok hasonlo a fenti formulahoz. ha kicsit rendezgetunk.:

$$
\frac{x \cdot y}{|x||y|} = cos(\alpha)
$$

ğŸ‘¨â€ğŸ«: Oooo igen. Ha egy altalanos skalarszorzatos vektorteret nezunk akkor abban igy tudjuk definialni ket vektor altal bezart szoget. Egy fontos megjegyzest azert tennek. A kozepiskolabol ismert skalarszorzasos keplet definialasahoz hasznaltatok a ket vektor altal bezart szoget. most viszont pont forditva csinaljuk. egy skalarszorzatos vektorterben a szoget definialjuk a skalarszorzas felhasznalasaval.


## Vektorrendszerek

ğŸ‘¨â€ğŸ«: Most egy kicsit egyes vektorrendszerekkel fogunk fouglalkozni es egy ket dologgal amit neha erdemes veluk csinalni. Legyen $ E = e_1, e_2 \dots e_n $ bazis a $V$ vektorterben, es fogjunk meg egy $ x \in V $ vektort. Mivel E vektorrendszer bazis volt igy a bazis definicioja miatt:

$$
\exists \lambda_0, \lambda_1, \dots \lambda_n : \lambda_0 x + \lambda_1 e_1 + \dots + \lambda_n e_n = 0 , \exists \lambda_i \neq 0, i \in {0, 1 \dots n}
$$

Namarmost azt feltehetjuk, hogy x egyutthatoja nem nulla. Ebbol pedig az alabbi kovetkezik:

$$
x = - \frac{\lambda_1}{\lambda_0} e_1 - \frac{\lambda_2}{\lambda_0} e_2 \dots - \frac{\lambda_n}{\lambda_0} e_n
$$
Na de mit is jelent ez?

ğŸ™‹â€â™‚ï¸: $ x $ -et fel tudjuk irni a bazisvektorok linearis kombinaciojakent?

ğŸ‘¨â€ğŸ«: Pontosan. Readasul az is igaz, hogy egyertelmuen tudjuk mindezt megtenni. Tehat nem lehet olyan, hogy egy vektort ugyanazokkal a bazisokkal kulonboz egyutthatokkal tudunk felirni. Az egyutthatok mindig egzertelmuek. Ennek a bizonyitasat rad hagyom.
{/* todo ennek leirhatom majd a megoldasat */}
Most pedig raterunk a vektorrendszereket erinto fogalmak bevezetesere. Ezek definiciok. Nagyjabol logikusak, de meg kell tanulni oket.
**Def.** $(v_i)_{i \in I}$ vektorrendszer ...

1. *OrtogonÃ¡lis, ha:* 
$$
\forall i, j \in I \text{-re} \quad \langle v_i, v_j \rangle = 0 \quad \text{ha} \quad i \neq j \quad 
$$

2. *NormalizÃ¡lt, ha:* 
$$
\forall i \in I \text{-re} \quad \|v_i\| = 1
$$

3. *OrtonormÃ¡lt, ha:* 
$$
\forall i, j \in I \text{-re} \quad \langle v_i, v_j \rangle = \delta_{ij}
$$

4. *Teljes, ha:* 
$$
\forall x \in V, \forall i \in I \text{-re} \quad \quad \langle x, v_i \rangle = 0 \Rightarrow x = 0
$$

### Gram-Schmidt-ortogonalizacio

ğŸ‘¨â€ğŸ«: Jooooo. Na most, hogy megvoltak ezek a fogalmaink nemelyekkel kezdunk is valamit. Azt tudjuk, hogy ugyanazt a vektorteret neha tobbfele bazissal is leirhatjuk. Es azt is tudjuk, hogy ezek kozott a bazisrendszerek kozott bijekcio all fenn. Szoval peldaul a sikon sem csak az altalunk megszokott, jol ismert kis bazisainkkal tudunk dolgozni. Lehet peldaul olyan bazisunk ahol a ket bazisvektor nem meroleges egymasra, vagy nem egyseg hosszuak. De persze ettol meg ok is jo bazisok... Bar azert mi sokszor jobban szeretunk a megszokott kis egyseghosszu meroleges bazisvektorkainkkal dolgozgatni. Szoval ha nem ilyen bazist talalunk akkor azt neha kenyelmes attranszformalni. Erre ad nagyszeru eszkozt a Gram-Schmidt-ortogonalizacio. Nezzuk is meg, hogy mukodik!

$$
\begin{aligned}
& \underline{\underline{\text{G-S-O}:}} \\
& \text{Vegyunk egy vektorrendszert } (v_1, v_2, \dots, v_n) \\
& \text{Az elsÅ‘ vektort egÃ©szen bÃ©kÃ©n hagyjuk.} \\
& \text{Csak levÃ¡gjuk egysÃ©gvektorra.} \\
& e_1 = \frac{v_1}{\|v_1\|} \\
& e_2 = \frac{v_2 - \langle v_2, e_1 \rangle e_1}{\| v_2 - \langle v_2, e_1 \rangle e_1 \|} \\
& e_k = \frac{v_k - \sum_{i=1}^{k-1} \langle v_k, e_i \rangle e_i}{\left\| v_k - \sum_{i=1}^{k-1} \langle v_k, e_i \rangle e_i \right\|}
\end{aligned}
$$

Es ezzel a folyamattal szepen vegigmegyunk az osszes vektoron a bazisban.

ğŸ™‹â€â™‚ï¸: Ha jol ertem nem csak ortogonalizalunk hanem normaljuk is a vektorainkat.

ğŸ‘¨â€ğŸ«: Igen, a nev nem emliti a normalast, de valoban. Igy kapunk az eredeti vektorrendszerbol egy olyat ami mar szep ortogonalis es normalt.

### Vektorok kifelytese

ğŸ‘¨â€ğŸ«: Na hasznaljuk is ki, hogy tudunk szep kis ortonormalt bazist kesziteni. Legyen $ (e_i)_{i = 1, \dots, n} $ ONB (ortonormalt bazis). Nezzuk meg, hogy az $ e_k$-hoz tartozo egyutthatot hogy tudjuk kifelyezni.
$$
\begin{aligned}
\langle e_k, x \rangle &= \left\langle e_k, \sum_{i=1}^n \lambda_i e_i \right\rangle \quad \text{Mivel: } x = \sum_{i=1}^n \lambda_i e_i \\
&= \sum_{i=1}^n \lambda_i \underbrace{\langle e_k, e_i \rangle}_{\delta_{ki}} = \lambda_k \\
\therefore \quad x &= \sum_{i=1}^n \langle e_i, x \rangle e_i \\[2em]
\end{aligned}
$$
Tehat $x$-et igy is kifelyezhetjuk. Ha mar ezt tudjuk nezzunk meg ezzel egy erdekes altalanositast. Nem mast altalanositunk itt mint a pithagorasz tetelt. Kicsit matekozzunk es aztan ertelmezzuk, hogy mit kapunk!


$$
\begin{aligned}
\|x\|^2 &= \left\langle \sum_{i=1}^n \langle e_i, x \rangle e_i, \; \sum_{j=1}^n \langle e_j, x \rangle e_j \right\rangle \\
&= \sum_{i=1}^n \overline{\langle e_i, x \rangle} \sum_{j=1}^n \langle e_j, x \rangle \langle e_i, e_j \rangle \\
&= \sum_{i,j=1}^n \overline{\langle e_i, x \rangle} \langle e_j, x \rangle \underbrace{\langle e_i, e_j \rangle}_{\delta_{ij}} \\
&= \sum_{i=1}^n \left| \langle e_i, x \rangle \right|^2
\end{aligned}
$$

ğŸ‘¨â€ğŸ«: Erted, hogy ez miert a pitagorasz tetel altalanositasa?

ğŸ™‹â€â™‚ï¸:Szerintem igen. Vegyuk a sikot a szokasos bazissal es skalarszorzassal. Ha ott elkepzelunk egy nyilacskat akkor annak a hossznegyzetet a kovetkezo keppen tudjuk kiszamolni.: Vegyuk a nyilacskanak az egyik bazisvektorra vett vetuletet, emeljuk negyzetre, jegyezzuk meg ezt a szamot. Most ugyanigy nezzuk meg a masik bazisvektorra vett vetuletet, ezt is emeljuk negyzetre es ezt a szamot is jegyezzuk meg. Most pedig ezt a ket szamot adjuk ossze. Ez igazabol a pitagorasz tetel. Es ugyan ezt csinaljuk most is csak tobb dimenzioban.

ğŸ‘¨â€ğŸ«: Nagyszeruuuu.



## Linearis lekepezesek

ğŸ‘¨â€ğŸ«: Elerkeztunk a tananyag egyik legfontosabb temajahoz. A linearis lekepezesekhez. $ U, V $ vektorterek. $ A: U \rightarrow V $ lekepezes linearis ha megfelel a kovetkezo ket kriteriumnak.:

$$
1. \forall x, y \in U \quad A(x+y) = A(x) + A(y) \\
2. \forall x \in U, \quad \forall \lambda \in \mathbb{K} \quad A(\lambda x) = \lambda A(x)
$$

Jo igazabol meg az is kell, hogy $ Dom(A) = U $. Tehat ha egy transzformalo masinanak kepzeljuk el akkor ez pluszban annyit tesz, hogy "legyszi minden vektort hadd tudjak benyomni a masinaba". Tudsz esetleg olyan lekepezest ami megfelelhet ezeknek a szabalyoknak?

ğŸ™‹â€â™‚ï¸: Talan az origo koruli forgatas $ \alpha $ szoggel.

ğŸ‘¨â€ğŸ«: Igen ez megfelelo. Peldaul a plinomok teren mondjuk jo meg a derivalas is vagy az indegralas korlatos zart intervallumon. Emellett ha mondjuk a szoki 3D-s terbol levetitesz az $xy$ sikra az is egy linearis lekepezes. De akar meg az is linearis lekepezes hogy nem csinalok semmit. Tehat $ A: V \rightarrow V, A(x) = x $ Ezt amugy identitasnak hivjuk.

ğŸ™‹â€â™‚ï¸: Meno.

ğŸ‘¨â€ğŸ«: Na jo. Folytassuk. Mi lenne ha megneznenk a linearis lekepezesek halmazat. Ezt valahogy formalisan igy irhatnank le.: $ L(U, V) = \{A: U \rightarrow V : A \quad \text{linearis} \} $ Definialjuk ket $ A, B \in L(U, V) $ linearis lekepezes osszeadasat es sakalrral valo szorzasaat $ \lambda \in \mathbb{K} $ kovetkezo modon. $ (u \in U) $:
$$
1. (A + B)(u) = A(u) + B(u) \\
2. (\lambda A)(u) = \lambda A(u)
$$


Most, hogy mar tudhunk ket linearis lekepezest osszeadni, es linearis lekepezest skalarral szorozni, belathatjuk, hogy $ L(U, V) $ vektorteret alkot. Mi most csak csak az osszeadas es a skalarral valo szorzas zartsagat fogjuk belatni. E vektorter tobbi axiomaja abbol kovetkezik, hogy $V$ maga is egy vekorter es igy a benne lako vekorokra igazak a vektorter axiomai (itt kihasznaljuk, hogy $ A(x) \in V $). Na gyerunk!

<ProofBox title="L(U, V) vektorteret alkot">
$$

\begin{aligned}
(A+B)(\lambda x + y) &= A(\lambda x + y) + B(\lambda x + y) \\
&= \lambda A(x) + A(y) + \lambda B(x) + B(y) \\
&= \lambda \big[ A(x) + B(x) \big] + A(y) + B(y) \\
&= \lambda (A+B)(x) + (A+B)(y) \\[3em]
(\eta \cdot A)(\lambda x + y) &= \eta \big( A(\lambda x + y) \big) \\
&= \eta \cdot \lambda A(x) + \eta \cdot A(y) \\
&= \lambda \cdot (\eta A)(x) + (\eta A)(y)
\\
\text{najo lassuk be a tobbit is: }\\
\\
& \text{1. KommutativitÃ¡s} \\
(A+B)(x) &= A(x) + B(x) \\
B(x) + A(x) &= (B+A)(x) \\[1em]
& \text{2. AsszociativitÃ¡s} \\
(A+(B+C))(x) &= A(x) + (B+C)(x) = A(x) + (B(x) + C(x)) \\
(A+B)(x) + C(x) &= ((A+B)+C)(x) \\[1em]
& \text{3. } 0 \text{ elem} \quad O(x) = 0_V \\
(A+O)(x) &= A(x) + O(x) \\
&= A(x) + 0_V = A(x) \\[1em]
& \text{4. Inverz} \\
\text{legyen } (-A)(x) &= -(A(x)) \\
(A+(-A))(x) &= A(x) + (-A)(x) \\
A(x) - A(x) &= 0 \\[1em]
& \text{5. } (1 \cdot A)(x) = 1 \cdot A(x) = A(x) \\[1em]
& \text{6. } (\lambda(A+B))(x) = \\
\lambda \cdot (A+B)(x) &= \lambda A(x) + \lambda B(x) \\
&= (\lambda A + \lambda B)(x) \\[1em]
& \text{7. } ((\lambda + \mu)A)(x) = \\
(\lambda + \mu) \cdot A(x) &= \lambda A(x) + \mu A(x) \\
(\lambda A)(x) + (\mu A)(x) &= (\lambda A + \mu A)(x) \\[1em]
& \text{8. } ((\lambda \mu)A)(x) = (\lambda \mu) A(x) \\
&= \lambda (\mu \cdot A(x)) = \lambda (\mu A)(x)
\end{aligned}

$$
</ProofBox>

ğŸ™‹â€â™‚ï¸: Hmm... Hat ez mar megint eleg absztrakt. Ez a linearis lekepezes valmi fuggvany szeruseg ami megeszik egy $ U $-ban lako vektort es kikop egy $ V $ -ben lako vektort, es annyit tudunk, hogy igaz ra 2 szabaly, hogy linearis legyen?

ğŸ‘¨â€ğŸ«: Kb. igen.

ğŸ™‹â€â™‚ï¸: Es ezzel hogy szamolunk? A rendes fuggvenyeknel meg volt legalbb adva valami fomula amibol ki tudtuk kovetkeztetni, hogy ha egy tetszoleges bemenetet etetunk meg a fuggvennyel akkor mit fog kikopni. Itt most nincs ilyen?

ğŸ‘¨â€ğŸ«: Szerencsenk van. Lehet ilyen formula szeruseget konstrualni a linearis lekepezesekbol. (Most kivetelesen az absztraktabb fogalombol csinalunk foldhozragadtabbat.) Ha emlekszel az $n$ dimenzios $U$ vektorterben, ahol $ (e_i)_{i \in \{1, \dots n\}}$ bazis egy $x \in U$ vektort fel tudtunk irni a kovetkezo keppen.:
$$
x = \alpha_1 e_1 + \alpha_2 e_2 + \dots + \alpha_n e_n
$$
Na akkor most hattassuk a linearis lekepezest $x$-en.:

$$
A(x) = (\alpha_1 e_1 + \alpha_2 e_2 + \dots + \alpha_n e_n) \\
\\
\quad \\
A(x) = \alpha_1 A (e_1) + \alpha_2 A (e_2) + \dots + \alpha_n A (e_n) \\

$$
Tehat ahhoz, hogy megtudjuk, hogy a linearis lekepezes mit csinal $x$-el, eleg tudjuk, hogy mit csinal a bazisvektorokkal, mivel az $\alpha_i$ egyutthatok nem valtoznak a trafo soran. Gyors kerdes. Melyik vektorterben van Az $A(e_i)$ vektor, ha $ A \in L(U, V)$?

<Question
  question="Melyik vektorterben van Az A(e_i) vektor?"
  options={[
    "U-ban",
    "V-ben",
    "ez nem egy vektor"
  ]}
  correctAnswerIndex={1}
/>

ğŸ‘¨â€ğŸ«: Azt mondtuk, hogy az $U$ vektorter $n$ dimenzios. Legyen $V$ vektorter $m$ dimenzios. Most, hogy mar tudjuk, hogy igazabol $A(e_i)$ egy $V$ beli vektor, mit mondhatunk el $A(e_i)$-rol?

ğŸ™‹â€â™‚ï¸: Hat akkor $A(e_i)$-t is fel lehet bontani $V$ beli bazisok linearkombinaciojara. Ha $V$-ben $ (f_j)_{j \in \{1, \dots m\}}$ bazis akkor:

$$
A(e_i) = \gamma_{1,i} f_1 + \gamma_{2,i} f_2 + \dots +\gamma_{m,i} f_m 
$$

ğŸ‘¨â€ğŸ«: Kiraly. Most lazan fogalmazva fogok csinalni egy tablazatot. Fogom minden $i$-re az altalad felirt kepletet $A(e_i)$-nek. Es a $\gamma$-s egyutthatokat berendezem egymas ala egy oszlopba. Most nem torodok az osszeadassal, es azzal sem hogy ott vannak az $f_j$ vektorok. Szimplan csinalok a $\gamma$ egyutthatokbol egy oszlopvektort, es ezt megcsinalom minden $i$-re, aztan az oszlopvektoraimat egymas melle teszem. Igy valami ilyesmit kapok.:

$$
\begin{aligned}
[A] &= \left[ \begin{array}{cccc}
\mid & \mid & & \mid \\
A(e_1) & A(e_2) & \dots & A(e_n) \\
\mid & \mid & & \mid
\end{array} \right] \\[2em]
\\
[A] &= \begin{bmatrix}
\gamma_{1,1} & \gamma_{1,2} & \dots & \gamma_{1,n} \\
\gamma_{2,1} & \gamma_{2,2} & \dots & \gamma_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{m,1} & \gamma_{m,2} & \dots & \gamma_{m,n}
\end{bmatrix}
\end{aligned}
$$

A $\gamma$-k helyett $A_{j,i}$ betuket fogok hasznalni (Vigyazat ha indexelve van akkor az a egy skalar, index enlkul pedig magat a linearis lekepzest ertem).

$$
\begin{aligned}
[A] &= \begin{bmatrix}
A_{1,1} & A_{1,2} & \dots & A_{1,n} \\
A_{2,1} & A_{2,2} & \dots & A_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
A_{m,1} & A_{m,2} & \dots & A_{m,n}
\end{bmatrix}
\end{aligned}
$$

Igy megalkottuk Az $A$ linearis lekepezes matrixat. A $[\space]$ jeloli, hogy ez mar egy matrix.

ğŸ™‹â€â™‚ï¸: Szoval most onkenyesen csinaltunk egy tablazatot abbol, hogy az $U$-ban levo bazisokkal mit csinal a linearis lekpezes. Miert jo ez nekunk, es miert pont igy rendeztuk el a tablazatunkat.

ğŸ‘¨â€ğŸ«: Igen ez igy elsore talan nem egyertelmu. A rovid valasz az, hogy ha majd vlamilyen manipulaciokat vegzunk az $A$ linearis lekepezessel (pl.: meg akarjuk nezni hogy mi tortenik ha ket lin. lekepezest hattatunk kompoziciokent) akkor ez a megfeleltetes lesz kenyelmes, igy fognak szepen kijonni a dolgok. Azt is szoktak mondani, hogy a 

{/* todo itt ird le az izomorfizmust a lin lekepezes es a matrixok kozott: https://www.youtube.com/watch?v=jtXEMAXvLr4  + az ilyen kanonikus dolgok is kellenek.*/}

ğŸ‘¨â€ğŸ«: Meg annyit erdemes megemliteni, hogy igazabol amit csinaltunk az annyi volt, hogy felhasznaltuk, hogy $U$ izomorf $\mathbb{K}^n$ -el es $V$ izomorf $\mathbb{K}^m$ -el. Tehat Valasztunk $U$ -ban es $V$ -ben is egy-egy bazist. Igy mukodik az izomorfizmus tehat vehetjuk ugy is, hogy 
{/* todo ez itt nagyon rossz */}


## Muveletek matrixokkal


<div id="spongebob-matrix-container" style={{display: 'flex', justifyContent: 'center', margin: '2rem 0', minHeight: '600px'}}></div>
<script is:inline src="/spongebob_matrix_sim.js"></script>


### 3D TranszformÃ¡ciÃ³ (BÃ³nusz)
PrÃ³bÃ¡ld ki a transzformÃ¡ciÃ³kat 3D-ben is Spongyabobon!
[Kattints ide a 3D szimulÃ¡ciÃ³ megtekintÃ©sÃ©hez](/spongebob_3d)

### Osszeadas, skalarral szorzas

ğŸ‘¨â€ğŸ«: Menjunk tovabb, es nezegessuk meg, hogy milyen muveleteket tudunk vegrehajtani linearis lekepzesek matrixaval. Eloszor nezzuk meg ket matrix osszeadasat es skalarral valo szorzasat. Van otleted hogy, hogyan adunk ossze ket matrixot? 

ğŸ™‹â€â™‚ï¸: Szamomra az lenne az intuitiv, ha elemenkent osszeadnank. Szorzasnal pedik minden elemet megszoroznank a kivant sakalarral.

ğŸ‘¨â€ğŸ«: Jo otlet. Valoban igy definialjuk matrixok osszeadasat es skalarral valo szorzasat. Egy dologra azert megis figyelnunk kell az osszeadasnal. Mi lehet ez?

<Question
  question="Mire kell figyeljunk matrixok osszeadasanal?"
  options={[
    "Az osszeadas csak a valos matrixokon ertelmezett",
    "Ez egy becsapos kerdes, akarmilyen matrixot ossze lehet adni",
    "Legyen a ket matrix alakja ugyan olyan (n x m -es) ",
    "Mindket linearis lekepezes (amikbol kepeztuk a matrixokat) ugyanabbol az U vt.-bol kepezzen ugyanabba a V vt.-be"
  ]}
  correctAnswerIndex={3}
/>

ğŸ™‹â€â™‚ï¸: Miert nem eleg, hogy ha ugyanolyan a ket matrix alakja. Ekkor ossze tudom oket adni, nem?

ğŸ‘¨â€ğŸ«: Igen, formalisan ossze lehet oket adni, de ha arra gondolsz hogy hogyan definialtuk a linearis lekepezesek osszeadasat akkor vilagossa valik a dolog. $ (A + B)(v) =  A(v) + B(v) $ Ha az $A$ es a $B$ lin. lekepezes mas vektorterbe kepez akkor $ A(v) + B(v) $ nem definialt. Na meg persze ha a lin. lekepezesek ertekkeszletekent szolgalo vektorterek is masak akkor meg nehezen tudjak a lin. lekepezesek "megenni" ugyanazt a $V$ vektort. Kicsit olyan ez mintha az 5 m/s-ot szeretned lehuteni. Vagy ossze akarnal adni 6kg-ot 4 oraval. Nincs igazan ertleme a dolognak.
{/* todo itt azert note h. programozasnal mi tortenik matrixok osszeadasakor */}

ğŸ‘¨â€ğŸ«: Tehat ha van $ A, B : U \rightarrow V $ lin. lekepezesunk es $ \lambda \in \mathbb{K} $ skalarunk akkor a kovetkezok igazak, $A$ es $B$ matrixaira:

$$
([A] + [B])_{ij} = [A]_{ij} + [B]_{ij}\\
\quad\\
(\lambda [A])_{ij} = \lambda [A]_{ij}
$$

Tehat valoban komponensenkent vegezzuk el a matrixok osszeadasat es skalarral valo szorzasat.

### Matrixok szorzasa


























## AsszociatÃ­v algebra

ğŸ‘¨â€ğŸ«: EmlÃ©kszel mÃ©g arra amikor definiÃ¡ltuk a skalÃ¡rszorzÃ¡st egy vektortÃ©ren? Eredetileg a vektortÃ©r elemeit csak Ã¶sszeadni tudtuk, meg skalÃ¡rral szorozni (jo persze volt inverz meg ilyesmik). Zavart, hogy nem tudtunk Ã¶sszeszorozni kÃ©t vektort. Ãgy bevezettÃ¼nk egy olyan fÃ¼ggvÃ©nyt ami megeszik kÃ©t vektort Ã©s kikÃ¶p egy skalÃ¡rt. Ez mÃ¡r egÃ©sz jÃ³! BÃ¡r azÃ©rt mÃ©g mindig zavar, hogy nem tudunk kÃ©t vektort Ã¶sszeszorozni Ãºgy, hogy egy harmadik vektort kapjunk.

ğŸ™‹â€â™‚ï¸: De vÃ¡rjunk az $n \times n $-es mÃ¡trixok vektorteret alkotnak, Ã©s mÃ¡trixokat tudunk Ã¶sszeszorozni. Akkor ha jÃ³l Ã©rtem akkor vannak vektorterek ahol mÃ¡r autÃ³matikusan mÅ±kÃ¶dik a vektorok szorzÃ¡sa.

ğŸ‘¨â€ğŸ«: Pontosan. Nagyon jÃ³ meglÃ¡tÃ¡s! Ha megfigyeljÃ¼k az nxn-es mÃ¡trixok szorzÃ¡sÃ¡ra vonatkozÃ³ jellemzÅ‘ket a kÃ¶vetkezÅ‘ket kapjuk: A, B, C mÃ¡trixok $ \lambda \mu $ skalÃ¡rok
1. $ A(BC) = (AB)C $
2. $ A(B + C) = AB + AC $
3. $ (A + B)C = AC + BC $
4. $ (\lambda A) (\mu B) = (\lambda \mu) (AB) $

NamÃ¡rmost nem csak a mÃ¡trixokra lehetnek igazak ezek a szabÃ¡lyok. Ha egy vektortÃ©r elemeire igazak a fÃ¶nti szabÃ¡lyok akkor azt mondhatjuk, hogy a vektortÃ©r elemei egy asszociatÃ­v algebrÃ¡t alkotnak. ÃltalÃ¡ban kis betÅ±vel Ã­rjuk a vektorokat, Ã­gy a szabÃ¡lyok egy asszociatÃ­v algebra lÃ©tezÃ©sÃ©hez:

1. $ a(bc) = (ab)c $
2. $ a(b + c) = ab + ac $
3. $ (a + b)c = ac + bc $
4. $ (\lambda a) (\mu b) = (\lambda \mu) (ab) $
(ha $\forall a, b, c \in V$ Ã©s $ \forall \lambda, \mu \in \mathbb{K}$ skalÃ¡rra igazak a szabÃ¡lyok)

<Question
  question="Az alÃ¡bbi hÃ¡rom kÃ¶zÃ¼l mi nem alkot asszociatÃ­v algebrÃ¡t?"
  options={[
    "Polinomok a standard polinom szorzÃ¡ssal",
    "R^3 ban lÃ©vÅ‘ vektorok vektoriÃ¡lis szorzÃ¡ssal",
    "f:R->R folytonos fÃ¼ggvÃ©nyek, a szorzÃ¡s a fÃ¼ggvÃ©nyÃ©rtÃ©kek normÃ¡l szorzÃ¡sa"
  ]}
  correctAnswerIndex={1}
/>

ğŸ‘¨â€ğŸ«: Ha visszatekintÃ¼nk a mÃ¡trixok szorzÃ¡sÃ¡ra egyes $n \times n $ mÃ¡trixoknak volt egy elÃ©g fontos tulajdonsÃ¡ga. EmlÃ©kszel mi volt ez?

ğŸ™‹â€â™‚ï¸:Hmmm... Ha egy mÃ¡trixnak nem zÃ©rus volt a determinÃ¡nsa akkor invertÃ¡lhatÃ³ volt, ha jÃ³l emlÃ©kszem.

ğŸ‘¨â€ğŸ«:Nagyon jÃ³Ã³. Ha visszaemlÃ©kezÃ¼nk $ A^{-1} $ az  $A$ mÃ¡trix inverze volt ha $ A^{-1}A = AA^{-1} = I $ Ahol $ I $ az $n \times n $-es
identitÃ¡smÃ¡trix volt.

Viszont ahhoz, hogy invertÃ¡ljunk kell az algebrÃ¡nkba egysÃ©gelem. Azokat az asszociatÃ­v algebrÃ¡kat amiben ilyen van azt egysÃ©gelemes asszociatÃ­v algebrÃ¡nak hÃ­vjuk (nagyon meglepÅ‘).
ÃltalÃ¡ban az egysÃ©gelemet $ e $-vel jelÃ¶ljÃ¼k. Az $A$ egysÃ©gelemes algebrÃ¡ra teljesÃ¼lnie kell, hogy: $ ae = ea = a : \forall a \in A $
Na mostmÃ¡r definiÃ¡lhatjuk az inverzet! $ a \in A $ invertÃ¡lhatÃ³, ha: $ \exists a^{-1} \in A : a^{-1}a = aa^{-1} = e $

VigyÃ¡zat az, hogy lÃ©tezik egysÃ©gelem az algebrÃ¡nkban nem jelenti azt, hogy feltÃ©tlen minden eleme az algebrÃ¡nknak invertÃ¡lhatÃ³! NÃ©zd pÃ©ldÃ¡ul a kÃ¶vetkezÅ‘ mÃ¡trixot:
$B =
\begin{bmatrix}
2 & 2\\
2 & 2
\end{bmatrix} $

### AsszociatÃ­v algebra spektruma

Egy egysÃ©gelemes asszociatÃ­v algebrÃ¡nak lÃ©tezik spektruma is. Az algebra egy $ a $ elemÃ©nek a spktruma azoknak a skalÃ¡roknak a halmaza amikre nem lÃ©tezik inverze $ a - \lambda e $-nak:

$$
sp(a) = \{ \lambda \in \mathbb{K} : \nexists (a - \lambda e)^{-1} \}
$$

Valahol mÃ¡r lÃ¡ttunk hasonlÃ³t. IsmerÅ‘s ez valahonnÃ©t?

ğŸ™‹â€â™‚ï¸:Sajnos nem.

ğŸ‘¨â€ğŸ«:Ha visszaemlÃ©kszÃ¼nk a sajÃ¡tÃ©rtÃ©k szÃ¡mÃ­tÃ¡sra, ott valami ilyesmi kÃ©pletÃ¼nk volt: $ \lambda $ sajÃ¡tÃ©rtÃ©ke $ A $-nak, ha $ det(A - \lambda I) = 0 $

ğŸ™‹â€â™‚ï¸:Aaa, mostmÃ¡r Ã©rtem. Ha nem lÃ©tezik $ (a - \lambda e) $-nek inverze az a mÃ¡trixok nyelvÃ©n azzal egyenÃ©rtÃ©kÅ±, hogy $ det(A - \lambda I) = 0 $ Mivel ha egy mÃ¡trixnak a determinÃ¡nsa $ 0 $ az azt jelenti, hogy a mÃ¡trix nem invertÃ¡lhatÃ³ (Ã©s vica verza).

ğŸ‘¨â€ğŸ«:Igen, a mÃ¡trixoknÃ¡l valÃ³ban a spektrum egyenÃ©rtÃ©kÅ± a sajÃ¡tÃ©rtÃ©kek halmazÃ¡val (vÃ©gtelen dimenziÃ³ban mÃ¡r nem csak a sajÃ¡tÃ©rtÃ©kekbÅ‘l Ã¡ll a spektrum, de ez most nem fontos).




## Jacobson-lemma:

ğŸ‘¨â€ğŸ«:Oooookes, most megnÃ©zÃ¼nk mengint egy kicsit absztraktabb Ã¡llÃ­tÃ¡st. IgÃ©rem lesz kicsit fÃ¶ldhÃ¶z ragadtabb kÃ¶vetkezmÃ©nye ami hasznunkra vÃ¡lhat.

Ãgy nagy vonalakban az Ã¡llÃ­tÃ¡s arrÃ³l szÃ³l, hogy ha van egy egysÃ©gelemes algebrÃ¡nk akkor aminek kÃ©t eleme a Ã©s b, akkor ab spektruma nem nagyon kÃ¼lÃ¶nbÃ¶zik ba spektrumÃ¡tÃ³l. Ez kicsit preciÃ­zebben.:

Van egy $ A $ egysÃ©gelemes ($ e $) algebrÃ¡nk. Ahol $ a, b \in A $. Ekkor:
$$
sp(ab) \cup \{0\} = sp(ba) \cup \{0\}
$$

NajÃ³ prÃ³bÃ¡ljuk meg belÃ¡tni ezt az Ã¡llÃ­tÃ¡st.



<ProofBox title="BizonyÃ­tÃ¡s: Jacobson-lemma">

$$
\begin{aligned}
& \text{Legyen } \lambda \notin \sigma(ab), \quad \lambda \in \mathbb{K}, \lambda \neq 0 \\
& \text{Legyen } c := (ab - \lambda e)^{-1} \\
& \text{TFH. } (ba - \lambda e)^{-1} = \alpha e + \beta bca \quad (\text{where } \alpha, \beta \in \mathbb{K}) \\
\\
& \text{Ahhoz hogy a feltÃ©tel teljesÃ¼ljÃ¶n: } (ba - \lambda e)(\alpha e + \beta bca) = e \\
\\
& \text{Kibontva:} \\
& \alpha ba + \beta babca - \lambda \alpha e - \lambda \beta bca = e \\
\\
& \text{VÃ¡lasszuk meg } \alpha \text{ Ã©rtÃ©kÃ©t:} \\
& -\lambda \alpha = 1 \implies \alpha = -\frac{1}{\lambda} \\
\\
& e\text{-vel egyszerÅ±sÃ­tve:} \\
& \alpha ba + \beta babca - \lambda \beta bca = 0 \\
& b (\alpha e + \beta abc - \beta \lambda c) a = 0 \\
& b \left( \alpha e + \beta (ab - \lambda e)c \right) a = 0 \\
\\
& \text{Mivel } (ab - \lambda e)c = e: \\
& b (\alpha e + \beta e) a = 0 \\
\\
& \text{legyen:} \beta = \frac{1}{\lambda}
\\
& b (0) a = 0 \\
& 0 = 0 \\
& \text{Ezzel az Ã¡llÃ­tÃ¡st belÃ¡ttuk az egyik irÃ¡nyba.}\\
\\
& \text{MÃ¡sik irÃ¡ny feltÃ©tele: } (\alpha e + \beta bca)(ba - \lambda e) = e \\
\\
& \text{Kibontva:} \\
& \alpha ba - \lambda \alpha e + \beta bcaba - \beta \lambda bca = e \\
\\
& a\text{ Ã©rtÃ©kÃ©t megvÃ¡lasztva:} \\
& -\lambda \alpha = 1 \implies \alpha = -\frac{1}{\lambda} \\
\\
& \text{EgyszerÅ±sÃ­tve:} \\
& \alpha ba + \beta bcaba - \beta \lambda bca = 0 \\
& b (\alpha e + \beta cab - \beta \lambda c) a = 0 \\
& b \left( \alpha e + \beta c(ab - \lambda e) \right) a = 0 \\
\\
& \text{Mivel:} \\
& c = (ab - \lambda e)^{-1}, \text{ we have } c(ab - \lambda e) = e: \\
& b (\alpha e + \beta e) a = 0 \\
\\
& \text{legyen:}\beta = \frac{1}{\lambda}\\
& b (0) a = 0 \\
& \text{Ezzel az Ã¡llÃ­tÃ¡st belÃ¡ttuk a mÃ¡sik irÃ¡nyba is.}\\
\end{aligned}
$$

</ProofBox>
{/* todo itt azert csinald meg h szep legyen es a vegese kell meg egy kis magyarazat */}


ğŸ‘¨â€ğŸ«: Ez a tÃ©tel Ã©s a hozzÃ¡ tartozÃ³ levezetÃ©s talÃ¡n kicsit absztraktnak tÅ±nhet, de van ennek a tÃ©telnek egy hasznos kÃ¶vetkezmÃ©nye. Igaz-e az hogy ha van kÃ©t $ n \times n $-es mÃ¡trixunk: $ A, B $ akkor: $AB = BA$ ?

<Question
  question="Mi a vÃ¡lasz a fÃ¶nti kÃ©rdÃ©sre?"
  options={[
    "Igen mindig",
    "Nem soha",
    "ÃltalÃ¡ban nem (lehet, hogy spec. esetben igen)"
  ]}
  correctAnswerIndex={2}
/>

ğŸ‘¨â€ğŸ«: Pontosan. A mÃ¡trixszorzÃ¡s nem feltÃ©tlen kommutatÃ­v. A Jacobson-lemma miatt viszont belÃ¡thatjuk, hogy $ sp(AB) = sp(BA) $ ha $ A, B \in n \times n $-es mÃ¡trix. Ez elÃ©g menÅ‘Å‘Å‘. HabÃ¡r A Ã©s B szorzata nem feltÃ©tlen kommutatÃ­v, a spektrumuk azonos. TehÃ¡t a sajÃ¡tÃ©rtÃ©keik megegyeznek.
LÃ¡ssuk ezt be gyorsan:

<ProofBox title="BizonyÃ­tÃ¡s: sp(AB) = sp(BA)">

$$
\begin{aligned}
& \text{Legyen } A, B \in \mathbb{R}^{n \times n} \text{ mÃ¡trix.} \\
& \text{Legyen } \lambda \in sp(AB). \\
\\
& 1. \text{ Ha } \lambda \neq 0: \\
& \quad \text{A Jacobson-lemma miatt: } \lambda \in sp(BA). \\
\\
& 2. \text{ Ha } \lambda = 0: \\
& \quad \text{Azt korÃ¡bbrÃ³l tudjuk, hogy: } \det(AB) = \prod_{i=1}^n \lambda_i \\
& \quad \text{TehÃ¡t ha lÃ©tezik egy } \lambda \text{ ami nulla, akkor: } \det(AB) = 0. \\
& \quad \text{De azt is tudjuk, hogy: } \det(AB) = \det(A)\det(B) = \det(BA). \\
& \quad \text{TehÃ¡t: } \det(BA) = 0 \implies \lambda \in sp(BA).
\end{aligned}
$$
</ProofBox>


## Spektralfelbontas

{/*  */}
















{/* OTLETEK */}
{/* todo ilyet majd csinalhatnek: https://ximera.osu.edu/la/LinearAlgebra/MAT-M-0023/main */}